import pandas as pd
import re
import requests
import uuid
import os
from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from io import BytesIO

app = FastAPI()

# æŒ‚è½½é™æ€ç›®å½•ï¼Œç”¨äºä¸‹è½½ç”Ÿæˆçš„ Excel
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
def read_root():
    return {"message": "Service is running!"}

@app.post("/process")
async def process_excel(request: Request):
    # 1. è·å–æ‰£å­ä¼ æ¥çš„å‚æ•°
    data = await request.json()
    file_url = data.get('file_url')
    
    if not file_url:
        return {"error": "No file_url provided"}
    
    try:
        # 2. ä¸‹è½½æ–‡ä»¶
        response = requests.get(file_url)
        response.raise_for_status()
        file_content = BytesIO(response.content)
    
        # 3. å¤„ç†é€»è¾‘ (ä½ çš„æ ¸å¿ƒä»£ç )
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        
        # --- æ•°æ®æ¸…æ´— ---
        new_columns = ['åºå·', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·', 'ä½¿ç”¨ç­çº§']
        df_clean = df.copy()
        if len(df_clean.columns) >= len(new_columns):
            df_clean.columns = new_columns
        else:
            df_clean = df_clean.iloc[:, :5]
            df_clean.columns = new_columns
    
        df_clean = df_clean.drop(0).reset_index(drop=True)
    
        def parse_class_info(class_str):
            classes = []
            pattern = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)äºº?ï¼‰'
            matches = re.findall(pattern, str(class_str))
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            pattern2 = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)ï¼‰'
            matches2 = re.findall(pattern2, str(class_str))
            for match in matches2:
                if not any(c[0] == match[0] for c in classes):
                    classes.append((match[0], int(match[1])))
            return classes
    
        processed_data = []
        for index, row in df_clean.iterrows():
            classes = parse_class_info(str(row['ä½¿ç”¨ç­çº§']))
            for class_name, student_count in classes:
                processed_data.append({
                    'æ•™æåç§°': row['æ•™æåç§°'],
                    'å‡ºç‰ˆç¤¾': row['å‡ºç‰ˆç¤¾'],
                    'ä¹¦å·': row['ä¹¦å·'],
                    'ç­çº§': class_name,
                    'äººæ•°': student_count
                })
    
        result_df = pd.DataFrame(processed_data)
        
        if result_df.empty:
            return {"error": "No valid data extracted"}
    
        # --- è¿™é‡Œçš„é€»è¾‘å’Œä½ åŸæ¥çš„ä¸€æ · ---
        def normalize_class_name_final(class_name):
            if 'äººï¼‰' in class_name or 'ï¼‰' in class_name:
                match = re.search(r'(2[45][^ï¼ˆï¼‰\s]+)', class_name)
                if match: return match.group(1)
            if 'çº§' in class_name and class_name.startswith(('24', '25')):
                year = class_name[:2]
                major = class_name[3:]
                if major.startswith('çº§'): major = major[1:]
                return year + major
            return class_name
    
        result_df['æ ‡å‡†åŒ–ç­çº§'] = result_df['ç­çº§'].apply(normalize_class_name_final)
        result_df_unique = result_df.drop_duplicates(subset=['æ ‡å‡†åŒ–ç­çº§', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()
        result_df_unique['å¹´ä»½'] = result_df_unique['æ ‡å‡†åŒ–ç­çº§'].str[:2]
        result_df_unique['ä¸“ä¸šç­çº§'] = result_df_unique['æ ‡å‡†åŒ–ç­çº§'].str[2:]
        result_df_sorted = result_df_unique.sort_values(['å¹´ä»½', 'ä¸“ä¸šç­çº§'], ascending=[False, True])
    
        unique_classes_sorted = result_df_sorted['æ ‡å‡†åŒ–ç­çº§'].drop_duplicates().tolist()
        class_numbers = {name: i for i, name in enumerate(unique_classes_sorted, 1)}
        result_df_sorted['åºå·'] = result_df_sorted['æ ‡å‡†åŒ–ç­çº§'].map(class_numbers)
        
        final_df = result_df_sorted[['åºå·', 'æ ‡å‡†åŒ–ç­çº§', 'ä¹¦å·', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'äººæ•°']].copy()
        final_df = final_df.rename(columns={'æ ‡å‡†åŒ–ç­çº§': 'ç­çº§', 'æ•™æåç§°': 'ä¹¦å'})
    
        # 4. ä¿å­˜æ–‡ä»¶
        filename = f"result_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)
    
        # 5. ç”Ÿæˆä¸‹è½½é“¾æ¥ (è‡ªåŠ¨è·å–å½“å‰åŸŸå)
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        
        # å¼ºåˆ¶ HTTPS (ä¸ºäº†é˜²æ­¢æ‰£å­æŠ¥é”™)
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)
    
        return {
            "download_url": download_url,
            "message": "success"
        }
    
    except Exception as e:

        return {"error": str(e)}
# ==========================================
# ğŸšª ç¬¬äºŒæ‰‡é—¨ï¼šå¤„ç†ã€äº¤é€šèŒé«˜2401ç­38äººã€‘æ ¼å¼# æ ¼
å¼#==========================================
async def process_winter_homework(request: Request):
    # 1. è·å–æ–‡ä»¶é“¾æ¥
    data = await request.json()
    file_url = data.get('file_url')
    if not file_url:
        return {"error": "è¯·æä¾›æ–‡ä»¶é“¾æ¥"}

    try:
        # 2. ä¸‹è½½æ–‡ä»¶
        response = requests.get(file_url)
        file_content = BytesIO(response.content)

        # 3. è¯»å– Excel
        df = pd.read_excel(file_content, sheet_name='Sheet1')

        # === ä½ çš„æ ¸å¿ƒé€»è¾‘å¼€å§‹ ===
        
        # æ¸…ç†æ•°æ®ï¼Œé‡æ–°è®¾ç½®åˆ—å
        new_columns = ['åºå·', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·', 'ä½¿ç”¨ç­çº§']
        df_clean = df.copy()
        # ç®€å•å®¹é”™ï¼šå¦‚æœåˆ—æ•°ä¸å¤Ÿï¼Œæˆªå–å‰5åˆ—ï¼›å¤Ÿçš„è¯ç›´æ¥æ”¹å
        if len(df_clean.columns) >= 5:
            df_clean = df_clean.iloc[:, :5]
        df_clean.columns = new_columns

        # åˆ é™¤ç¬¬ä¸€è¡Œï¼ˆåŸæ¥çš„åˆ—åè¡Œï¼‰
        df_clean = df_clean.drop(0).reset_index(drop=True)

        # å®šä¹‰è§£æå‡½æ•° (ä½ çš„æ–°æ­£åˆ™é€»è¾‘)
        def parse_class_info_new(class_str):
            classes = []
            s = str(class_str) # å¼ºåˆ¶è½¬å­—ç¬¦ä¸²é˜²æ­¢æŠ¥é”™
            
            # æ¨¡å¼1ï¼š2401ç­40äºº
            pattern = r'(\d+ç­)\s*(\d+)äºº'
            matches = re.findall(pattern, s)
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            # æ¨¡å¼2ï¼šå¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯• 2401ç­40 (æ— "äºº"å­—)
            if not classes:
                pattern2 = r'(\d+ç­)\s*(\d+)'
                matches2 = re.findall(pattern2, s)
                for match in matches2:
                    classes.append((match[0], int(match[1])))
            
            return classes

        processed_data = []

        # éå†å¤„ç†
        for index, row in df_clean.iterrows():
            textbook_name = row['æ•™æåç§°']
            publisher = row['å‡ºç‰ˆç¤¾']
            isbn = row['ä¹¦å·']
            class_info = row['ä½¿ç”¨ç­çº§']
            
            # ç©ºå€¼è·³è¿‡
            if pd.isna(class_info) or str(class_info).strip() == '':
                continue
            
            classes = parse_class_info_new(class_info)
            
            for class_name, student_count in classes:
                processed_data.append({
                    'æ•™æåç§°': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'ä¹¦å·': isbn,
                    'ç­çº§': class_name,
                    'äººæ•°': student_count
                })

        # ç”Ÿæˆç»“æœ DataFrame
        result_df = pd.DataFrame(processed_data)
        
        if result_df.empty:
            return {"error": "æœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ æ–‡ä»¶çš„æ ¼å¼ã€‚"}

        # === æ’åºä¸ç¼–å·é€»è¾‘ ===
        
        # æå–æ•°å­—ç”¨äºæ’åº (2401ç­ -> 2401)
        # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²å†æ›¿æ¢ï¼Œé˜²æ­¢éå­—ç¬¦ä¸²ç±»å‹æŠ¥é”™
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§'].astype(str).str.replace('ç­', '', regex=False)
        # è¿‡æ»¤æ‰éæ•°å­—çš„è¡Œï¼ˆé˜²æ­¢è„æ•°æ®æŠ¥é”™ï¼‰ï¼Œè½¬ä¸ºint
        result_df = result_df[result_df['ç­çº§ç¼–å·æ•°å­—'].str.isnumeric()] 
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§ç¼–å·æ•°å­—'].astype(int)

        # æ’åº
        result_df_sorted = result_df.sort_values('ç­çº§ç¼–å·æ•°å­—', ascending=True)

        # å»é‡
        result_df_unique = result_df_sorted.drop_duplicates(subset=['ç­çº§', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()

        # ç”Ÿæˆç®€æ˜“ç¼–å· (1, 2, 3...)
        unique_classes_sorted = result_df_unique['ç­çº§'].drop_duplicates().tolist()
        class_numbers = {name: i for i, name in enumerate(unique_classes_sorted, 1)}
        
        result_df_unique['ç¼–å·'] = result_df_unique['ç­çº§'].map(class_numbers)

        # æ•´ç†æœ€ç»ˆåˆ—é¡ºåº
        final_df = result_df_unique[['ç¼–å·', 'ç­çº§', 'äººæ•°', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']].reset_index(drop=True)

        # === ä½ çš„æ ¸å¿ƒé€»è¾‘ç»“æŸ ===

        # 4. ä¿å­˜æ–‡ä»¶å¹¶ç”Ÿæˆé“¾æ¥
        filename = f"winter_hw_{uuid.uuid4()}.xlsx" # è¿™é‡Œçš„åå­—å¯ä»¥æ”¹
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)

        # è·å–ä¸‹è½½é“¾æ¥
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)

        return {
            "download_url": download_url,
            "message": "å¯’å‡ä½œä¸šå¤„ç†å®Œæˆ"
        }

    except Exception as e:
        return {"error": f"å¤„ç†å‡ºé”™: {str(e)}"}
