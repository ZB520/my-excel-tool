import pandas as pd
import re
import requests
import uuid
import os
import traceback
from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from io import BytesIO

app = FastAPI()

# æŒ‚è½½é™æ€ç›®å½•
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
def read_root():
    return {"message": "Service is running! All systems go."}

# =================================================================
# å…¬å…±å‡½æ•°ï¼šè‡ªåŠ¨è¯†åˆ«åˆ—å (Fuzzy Column Mapping)
# =================================================================
def find_columns_by_keywords(df_columns):
    """
    è¾“å…¥ DataFrame çš„åˆ—ååˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œæ˜ å°„æ ‡å‡†å­—æ®µååˆ°å®é™…åˆ—åã€‚
    """
    column_keywords = {
        'target_book_name': ['æ•™æ', 'ä¹¦å', 'åç§°', 'è¯¾æœ¬'],
        'target_publisher': ['å‡ºç‰ˆ', 'ç‰ˆç¤¾'],
        'target_isbn': ['ä¹¦å·', 'ISBN', 'isbn', 'æ ‡å‡†å·'],
        'target_class': ['ç­çº§', 'ä½¿ç”¨ç­çº§', 'é€‚ç”¨å¯¹è±¡', 'èŒƒå›´']
    }
    
    found_cols = {}
    for col in df_columns:
        col_str = str(col).strip()
        for key, keywords in column_keywords.items():
            if key not in found_cols and any(kw in col_str for kw in keywords):
                found_cols[key] = col
    
    return found_cols

# ==========================================
# ğŸšª ç¬¬ä¸€æ‰‡é—¨ï¼šå¤„ç†ã€ä¹¦å•ã€‘æ ¼å¼ (process_excel)
# ==========================================
@app.post("/process")
async def process_excel(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    
    if not file_url:
        return {"error": "No file_url provided"}
    
    try:
        response = requests.get(file_url)
        response.raise_for_status()
        file_content = BytesIO(response.content)
        
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        found_cols = find_columns_by_keywords(df.columns)
        
        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
             return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        def parse_class_info(class_str):
            classes = []
            pattern = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)äºº?ï¼‰'
            matches = re.findall(pattern, str(class_str))
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            pattern2 = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)ï¼‰'
            matches2 = re.findall(pattern2, str(class_str))
            for match in matches2:
                if not any(c[0] == match[0] for c in classes):
                    classes.append((match[0], int(match[1])))
            return classes
        
        processed_data = []
        for index, row in df.iterrows():
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            classes = parse_class_info(str(class_str))
            for class_name, student_count in classes:
                processed_data.append({
                    'åŸå§‹ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })
        
        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "No valid data extracted"}

        def normalize_class_name_final(class_name):
            if 'äººï¼‰' in class_name or 'ï¼‰' in class_name:
                match = re.search(r'(2[45][^ï¼ˆï¼‰\s]+)', class_name)
                if match: return match.group(1)
            if 'çº§' in class_name and class_name.startswith(('24', '25')):
                year = class_name[:2]
                major = class_name[3:]
                if major.startswith('çº§'): major = major[1:]
                return year + major
            return class_name

        result_df['ç­çº§'] = result_df['åŸå§‹ç­çº§'].apply(normalize_class_name_final)
        
        result_df_unique = result_df.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()
        
        result_df_unique['å¹´ä»½'] = result_df_unique['ç­çº§'].str[:2]
        result_df_unique['ä¸“ä¸šç­çº§'] = result_df_unique['ç­çº§'].str[2:]
        result_df_sorted = result_df_unique.sort_values(['å¹´ä»½', 'ä¸“ä¸šç­çº§'], ascending=[False, True])
        
        # ==================== ä¿®å¤é€»è¾‘ START ====================
        # è·å–å”¯ä¸€çš„ç­çº§åˆ—è¡¨ï¼ˆä¿æŒæ’åºé¡ºåºï¼‰
        unique_classes = result_df_sorted['ç­çº§'].drop_duplicates().tolist()
        # åˆ›å»ºæ˜ å°„å­—å…¸ï¼š{ '24æŠ¤ç†1ç­': 1, '24æŠ¤ç†2ç­': 2, ... }
        class_map = {name: i for i, name in enumerate(unique_classes, 1)}
        # æ˜ å°„åºå·
        result_df_sorted['åºå·'] = result_df_sorted['ç­çº§'].map(class_map)
        # ==================== ä¿®å¤é€»è¾‘ END ======================
        
        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        for col in final_cols:
            if col not in result_df_sorted.columns:
                result_df_sorted[col] = ""
                
        final_df = result_df_sorted[final_cols]
        
        filename = f"result_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)
        
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)
        
        return {"download_url": download_url, "message": "success"}
    
    except Exception as e:
        traceback.print_exc()
        return {"error": str(e)}


# ==========================================
# ğŸšª ç¬¬äºŒæ‰‡é—¨ï¼šå¤„ç†ã€å¯’å‡ä½œä¸šã€‘æ ¼å¼ (process_winter_homework)
# ==========================================
@app.post("/process_winter_homework")
async def process_winter_homework(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    if not file_url:
        return {"error": "è¯·æä¾›æ–‡ä»¶é“¾æ¥"}

    try:
        response = requests.get(file_url)
        file_content = BytesIO(response.content)
        
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        found_cols = find_columns_by_keywords(df.columns)

        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
            return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        def parse_class_info_new(class_str):
            classes = []
            s = str(class_str)
            pattern = r'(\d+ç­)\s*(\d+)äºº'
            matches = re.findall(pattern, s)
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            if not classes:
                pattern2 = r'(\d+ç­)\s*(\d+)'
                matches2 = re.findall(pattern2, s)
                for match in matches2:
                    classes.append((match[0], int(match[1])))
            return classes

        processed_data = []
        for index, row in df.iterrows():
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            if pd.isna(class_str) or str(class_str).strip() == '':
                continue
            
            classes = parse_class_info_new(class_str)
            for class_name, student_count in classes:
                processed_data.append({
                    'ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })

        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "æœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç­çº§åˆ—æ ¼å¼"}

        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§'].astype(str).str.replace('ç­', '', regex=False)
        result_df = result_df[result_df['ç­çº§ç¼–å·æ•°å­—'].str.isnumeric()] 
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§ç¼–å·æ•°å­—'].astype(int)
        
        result_df_sorted = result_df.sort_values('ç­çº§ç¼–å·æ•°å­—', ascending=True)
        result_df_unique = result_df_sorted.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()

        # ==================== ä¿®å¤é€»è¾‘ START ====================
        # 1. æå–æ‰€æœ‰ä¸é‡å¤çš„ç­çº§ï¼Œä¿æŒæ’åºé¡ºåº
        unique_classes = result_df_unique['ç­çº§'].drop_duplicates().tolist()
        # 2. ç”Ÿæˆç­çº§IDå­—å…¸ï¼š{'101ç­': 1, '102ç­': 2, ...}
        class_map = {name: i for i, name in enumerate(unique_classes, 1)}
        # 3. å°†IDæ˜ å°„å›æ•°æ®æ¡†
        result_df_unique['åºå·'] = result_df_unique['ç­çº§'].map(class_map)
        # ==================== ä¿®å¤é€»è¾‘ END ======================

        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        for col in final_cols:
            if col not in result_df_unique.columns:
                result_df_unique[col] = ""

        final_df = result_df_unique[final_cols].reset_index(drop=True)

        filename = f"winter_hw_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)

        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)

        return {"download_url": download_url, "message": "å¯’å‡ä½œä¸šå¤„ç†å®Œæˆ"}

    except Exception as e:
        traceback.print_exc()
        return {"error": f"å¤„ç†å‡ºé”™: {str(e)}"}

# ==========================================
# ğŸšª ç¬¬ä¸‰æ‰‡é—¨ï¼šå¤„ç†ã€è¥¿æ¹–èŒé«˜ã€‘æ ¼å¼ (æœ€ç»ˆç‰ˆ)
# ==========================================
@app.post("/process_westlake")
async def process_westlake(request: Request):
    # 1. è·å–æ–‡ä»¶é“¾æ¥
    data = await request.json()
    file_url = data.get('file_url')
    if not file_url:
        return {"error": "è¯·æä¾›æ–‡ä»¶é“¾æ¥"}

    try:
        # 2. ä¸‹è½½æ–‡ä»¶
        response = requests.get(file_url)
        response.raise_for_status()
        file_content = BytesIO(response.content)

        # 3. è¯»å– Excel
        df = pd.read_excel(file_content, sheet_name='Sheet1')

        # === æ ¸å¿ƒå¤„ç†é€»è¾‘ (æ‚¨çš„æœ€ç»ˆç‰ˆä»£ç ) ===

        # æ¸…ç†æ•°æ®ï¼Œé‡æ–°è®¾ç½®åˆ—å
        new_columns = ['åºå·', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·', 'ä½¿ç”¨ç­çº§']
        df_clean = df.copy()
        
        # å®¹é”™ï¼šç¡®ä¿åˆ—æ•°è¶³å¤Ÿ
        if len(df_clean.columns) >= 5:
            df_clean = df_clean.iloc[:, :5]
        df_clean.columns = new_columns

        # åˆ é™¤ç¬¬ä¸€è¡Œ
        df_clean = df_clean.drop(0).reset_index(drop=True)

        # å®šä¹‰è§£æå‡½æ•° (åŸºäºæ‚¨çš„æœ€ç»ˆç‰ˆé€»è¾‘)
        def parse_class_info_new_format(class_str):
            classes = []
            s = str(class_str)
            
            # å…ˆæ¸…ç†å­—ç¬¦ä¸²ï¼Œå»æ‰æ‹¬å·å’Œæ‹¬å·é‡Œçš„å†…å®¹
            cleaned_str = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', s)  # å»æ‰ä¸­æ–‡æ‹¬å·å†…å®¹
            cleaned_str = re.sub(r'\([^)]*\)', '', cleaned_str)  # å»æ‰è‹±æ–‡æ‹¬å·å†…å®¹
            cleaned_str = cleaned_str.strip(' ã€ï¼Œ,')  # å»æ‰é¦–å°¾çš„åˆ†éš”ç¬¦
            
            # æ ¼å¼1ï¼šèŒ¶è‰º231-45 â†’ 23èŒ¶è‰º1 (ä¸“ä¸š+å¹´ä»½ç­çº§-äººæ•°)
            pattern1 = r'([\u4e00-\u9fa5]+)(\d{2})(\d+)(?:-(\d+))?'
            matches1 = re.findall(pattern1, cleaned_str)
            
            # æ ¼å¼2ï¼šç”µ251-45 â†’ 25ç”µ1 (ä¸“ä¸š+å¹´ä»½ç­çº§-äººæ•°)
            pattern2 = r'([\u4e00-\u9fa5]*)(\d{2})(\d+)(?:-(\d+))?'
            matches2 = re.findall(pattern2, cleaned_str)

            # å¤„ç†æ ¼å¼1
            for match in matches1:
                major, year, class_num, count = match
                class_name = f"{year}{major}{class_num}"
                cnt = int(count) if count else None
                classes.append((class_name, cnt))
            
            # å¤„ç†æ ¼å¼2 (å¦‚æœæ ¼å¼1æ²¡åŒ¹é…åˆ°ï¼Œæˆ–è€…æœ‰æ··åˆæƒ…å†µ)
            # æ³¨æ„ï¼šmatches2 ä¹Ÿä¼šåŒ¹é…åˆ° matches1 çš„æƒ…å†µï¼Œæ‰€ä»¥éœ€è¦å»é‡æˆ–é€»è¾‘åˆ¤æ–­
            # ä½†æ‚¨çš„åŸä»£ç æ˜¯åˆ†å¼€è¿½åŠ çš„ï¼Œè¿™é‡Œä¿æŒåŸé€»è¾‘
            if not matches1: 
                for match in matches2:
                    major, year, class_num, count = match
                    if not major: major = "ç”µ" # é»˜è®¤ä¸“ä¸š
                    
                    class_name = f"{year}{major}{class_num}"
                    
                    # é¿å…é‡å¤æ·»åŠ  (å› ä¸º pattern2 åŒ…å«äº† pattern1 çš„éƒ¨åˆ†ç‰¹å¾)
                    if not any(c[0] == class_name for c in classes):
                        cnt = int(count) if count else None
                        classes.append((class_name, cnt))
            
            # æ ¼å¼3ï¼šçº¯æ•°å­— 231-45 â†’ 23ç”µ1
            if not classes:
                pattern3 = r'(\d{2})(\d+)(?:-(\d+))?'
                matches3 = re.findall(pattern3, cleaned_str)
                for match in matches3:
                    year, class_num, count = match
                    class_name = f"{year}ç”µ{class_num}"
                    if not any(c[0] == class_name for c in classes):
                        cnt = int(count) if count else None
                        classes.append((class_name, cnt))

            # æ ¼å¼4ï¼šä¸‰ä½æ•°å­— 251 â†’ 25ç”µ1
            if not classes:
                pattern4 = r'(\d{3})(?:-(\d+))?'
                matches4 = re.findall(pattern4, cleaned_str)
                for match in matches4:
                    full_num, count = match
                    if len(full_num) == 3:
                        year = full_num[:2]
                        class_num = full_num[2:]
                        class_name = f"{year}ç”µ{class_num}"
                        if not any(c[0] == class_name for c in classes):
                            cnt = int(count) if count else None
                            classes.append((class_name, cnt))
                            
            return classes

        # å®šä¹‰æ’åºå‡½æ•°
        def get_class_sort_key(class_name):
            # åŒ¹é…å¹´ä»½+ä¸“ä¸š+ç­å·
            match = re.search(r'^(\d{2})', str(class_name))
            if match:
                year = int(match.group(1))
                class_num_match = re.search(r'(\d+)$', str(class_name))
                if class_num_match:
                    class_num = int(class_num_match.group(1))
                    return year * 100 + class_num
                return year * 100
            return 999999

        processed_data = []
        for index, row in df_clean.iterrows():
            textbook_name = row['æ•™æåç§°']
            publisher = row['å‡ºç‰ˆç¤¾']
            isbn = row['ä¹¦å·']
            class_info = row['ä½¿ç”¨ç­çº§']
            
            if pd.isna(class_info) or str(class_info).strip() == '':
                continue
            
            classes = parse_class_info_new_format(class_info)
            
            for class_name, student_count in classes:
                processed_data.append({
                    'æ•™æåç§°': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'ä¹¦å·': isbn,
                    'ç­çº§': class_name,
                    'äººæ•°': student_count
                })

        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "æœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®"}

        # æ’åº
        result_df['æ’åºé”®'] = result_df['ç­çº§'].apply(get_class_sort_key)
        result_df_sorted = result_df.sort_values('æ’åºé”®', ascending=True)

        # å»é‡
        result_df_unique = result_df_sorted.drop_duplicates(subset=['ç­çº§', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()

        # ç¼–å·
        unique_classes = result_df_unique['ç­çº§'].drop_duplicates().tolist()
        class_map = {name: i for i, name in enumerate(unique_classes, 1)}
        result_df_unique['ç¼–å·'] = result_df_unique['ç­çº§'].map(class_map)

        # æœ€ç»ˆåˆ—é¡ºåº (æ³¨æ„ï¼šæ‚¨ä»£ç é‡Œå»æ‰äº†æ’åºé”®)
        final_df = result_df_unique[['ç¼–å·', 'ç­çº§', 'äººæ•°', 'æ•™æåç§°', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']].reset_index(drop=True)

        # === ä¿å­˜æ–‡ä»¶ ===
        filename = f"westlake_final_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)

        # ç”Ÿæˆé“¾æ¥
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)

        return {"download_url": download_url, "message": "è¥¿æ¹–èŒé«˜(æœ€ç»ˆç‰ˆ)å¤„ç†å®Œæˆ"}

    except Exception as e:
        return {"error": f"å¤„ç†å‡ºé”™: {str(e)}"}
