import pandas as pd
import re
import requests
import uuid
import os
import traceback
from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from io import BytesIO

app = FastAPI()

# æŒ‚è½½é™æ€ç›®å½•
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
def read_root():
    return {"message": "Service is running! All systems go."}

# =================================================================
# å…¬å…±å‡½æ•°ï¼šè‡ªåŠ¨è¯†åˆ«åˆ—å (Fuzzy Column Mapping)
# =================================================================
def find_columns_by_keywords(df_columns):
    """
    è¾“å…¥ DataFrame çš„åˆ—ååˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œæ˜ å°„æ ‡å‡†å­—æ®µååˆ°å®é™…åˆ—åã€‚
    """
    # å®šä¹‰å…³é”®è¯æ˜ å°„è¡¨
    column_keywords = {
        'target_book_name': ['æ•™æ', 'ä¹¦å', 'åç§°', 'è¯¾æœ¬'],
        'target_publisher': ['å‡ºç‰ˆ', 'ç‰ˆç¤¾'],
        'target_isbn': ['ä¹¦å·', 'ISBN', 'isbn', 'æ ‡å‡†å·'],
        'target_class': ['ç­çº§', 'ä½¿ç”¨ç­çº§', 'é€‚ç”¨å¯¹è±¡', 'èŒƒå›´']
    }
    
    found_cols = {}
    for col in df_columns:
        col_str = str(col).strip()
        for key, keywords in column_keywords.items():
            # åªè¦åŒ…å«å…³é”®è¯ï¼Œä¸”è¯¥å­—æ®µè¿˜æ²¡æ‰¾åˆ°ï¼Œå°±è®°å½•ä¸‹æ¥
            if key not in found_cols and any(kw in col_str for kw in keywords):
                found_cols[key] = col
    
    return found_cols

# ==========================================
# ğŸšª ç¬¬ä¸€æ‰‡é—¨ï¼šå¤„ç†ã€ä¹¦å•ã€‘æ ¼å¼ (process_excel)
# åœºæ™¯ï¼šå¤„ç†å¤æ‚ç­çº§å (å¦‚: 24çº§æŠ¤ç†1ç­)
# ==========================================
@app.post("/process")
async def process_excel(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    
    if not file_url:
        return {"error": "No file_url provided"}
    
    try:
        response = requests.get(file_url)
        response.raise_for_status()
        file_content = BytesIO(response.content)
        
        # 1. è¯»å– Excel
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        
        # 2. è‡ªåŠ¨è¯†åˆ«åˆ—å
        found_cols = find_columns_by_keywords(df.columns)
        
        # æ£€æŸ¥æ ¸å¿ƒåˆ—æ˜¯å¦å­˜åœ¨
        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
             return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        # 3. å®šä¹‰è¯¥æ¥å£ç‰¹æœ‰çš„è§£æé€»è¾‘ (é’ˆå¯¹å¤æ‚ç­çº§å)
        def parse_class_info(class_str):
            classes = []
            # åŒ¹é… "24æŠ¤ç†1ç­ï¼ˆ45äººï¼‰"
            pattern = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)äºº?ï¼‰'
            matches = re.findall(pattern, str(class_str))
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            # åŒ¹é… "24æŠ¤ç†1ç­ï¼ˆ45ï¼‰" - è¡¥å……åŒ¹é…
            pattern2 = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)ï¼‰'
            matches2 = re.findall(pattern2, str(class_str))
            for match in matches2:
                if not any(c[0] == match[0] for c in classes):
                    classes.append((match[0], int(match[1])))
            return classes
        
        processed_data = []
        for index, row in df.iterrows():
            # è·å–æ•°æ®
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            classes = parse_class_info(str(class_str))
            for class_name, student_count in classes:
                processed_data.append({
                    'åŸå§‹ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })
        
        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "No valid data extracted"}

        # 4. ç­çº§åæ ‡å‡†åŒ– (è¯¥æ¥å£ç‰¹æœ‰é€»è¾‘)
        def normalize_class_name_final(class_name):
            if 'äººï¼‰' in class_name or 'ï¼‰' in class_name:
                match = re.search(r'(2[45][^ï¼ˆï¼‰\s]+)', class_name)
                if match: return match.group(1)
            if 'çº§' in class_name and class_name.startswith(('24', '25')):
                year = class_name[:2]
                major = class_name[3:]
                if major.startswith('çº§'): major = major[1:]
                return year + major
            return class_name

        result_df['ç­çº§'] = result_df['åŸå§‹ç­çº§'].apply(normalize_class_name_final)
        
        # 5. å»é‡ä¸æ’åº
        result_df_unique = result_df.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()
        
        # æå–å¹´ä»½å’Œä¸“ä¸šè¿›è¡Œæ’åº
        result_df_unique['å¹´ä»½'] = result_df_unique['ç­çº§'].str[:2]
        result_df_unique['ä¸“ä¸šç­çº§'] = result_df_unique['ç­çº§'].str[2:]
        result_df_sorted = result_df_unique.sort_values(['å¹´ä»½', 'ä¸“ä¸šç­çº§'], ascending=[False, True])
        
        # ç”Ÿæˆåºå·
        result_df_sorted['åºå·'] = range(1, len(result_df_sorted) + 1)
        
        # 6. æŒ‰æŒ‡å®šé¡ºåºè¾“å‡º
        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        
        # è¡¥é½å¯èƒ½ç¼ºå¤±çš„åˆ—
        for col in final_cols:
            if col not in result_df_sorted.columns:
                result_df_sorted[col] = ""
                
        final_df = result_df_sorted[final_cols]
        
        # 7. ä¿å­˜
        filename = f"result_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)
        
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)
        
        return {"download_url": download_url, "message": "success"}
    
    except Exception as e:
        traceback.print_exc()
        return {"error": str(e)}


# ==========================================
# ğŸšª ç¬¬äºŒæ‰‡é—¨ï¼šå¤„ç†ã€å¯’å‡ä½œä¸šã€‘æ ¼å¼ (process_winter_homework)
# åœºæ™¯ï¼šå¤„ç†ç®€å•æ•°å­—ç­çº§å (å¦‚: 101ç­)
# ==========================================
@app.post("/process_winter_homework")
async def process_winter_homework(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    if not file_url:
        return {"error": "è¯·æä¾›æ–‡ä»¶é“¾æ¥"}

    try:
        response = requests.get(file_url)
        file_content = BytesIO(response.content)
        
        # 1. è¯»å– Excel
        df = pd.read_excel(file_content, sheet_name='Sheet1')

        # 2. è‡ªåŠ¨è¯†åˆ«åˆ—å (ä½¿ç”¨å…¬å…±é€»è¾‘)
        found_cols = find_columns_by_keywords(df.columns)

        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
            return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        # 3. å®šä¹‰è¯¥æ¥å£ç‰¹æœ‰çš„è§£æé€»è¾‘ (é’ˆå¯¹ "101ç­ 40äºº" è¿™ç§æ ¼å¼)
        def parse_class_info_new(class_str):
            classes = []
            s = str(class_str)
            # æ¨¡å¼1: "101ç­ 45äºº"
            pattern = r'(\d+ç­)\s*(\d+)äºº'
            matches = re.findall(pattern, s)
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            if not classes:
                # æ¨¡å¼2: "101ç­ 45" (æ²¡æœ‰'äºº'å­—)
                pattern2 = r'(\d+ç­)\s*(\d+)'
                matches2 = re.findall(pattern2, s)
                for match in matches2:
                    classes.append((match[0], int(match[1])))
            return classes

        processed_data = []
        
        for index, row in df.iterrows():
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            if pd.isna(class_str) or str(class_str).strip() == '':
                continue
            
            classes = parse_class_info_new(class_str)
            for class_name, student_count in classes:
                processed_data.append({
                    'ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })

        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "æœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç­çº§åˆ—æ ¼å¼"}

        # 4. æ’åºé€»è¾‘ (é’ˆå¯¹æ•°å­—ç­çº§æ’åº)
        # æå–æ•°å­—è¿›è¡Œæ’åº: 101ç­ -> 101
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§'].astype(str).str.replace('ç­', '', regex=False)
        result_df = result_df[result_df['ç­çº§ç¼–å·æ•°å­—'].str.isnumeric()] 
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§ç¼–å·æ•°å­—'].astype(int)
        
        result_df_sorted = result_df.sort_values('ç­çº§ç¼–å·æ•°å­—', ascending=True)

        # 5. å»é‡
        result_df_unique = result_df_sorted.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()

        # ç”Ÿæˆåºå·
        result_df_unique['åºå·'] = range(1, len(result_df_unique) + 1)

        # 6. æŒ‰æŒ‡å®šé¡ºåºè¾“å‡º
        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        
        for col in final_cols:
            if col not in result_df_unique.columns:
                result_df_unique[col] = ""

        final_df = result_df_unique[final_cols].reset_index(drop=True)

        # 7. ä¿å­˜ä¸è¿”å›
        filename = f"winter_hw_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)

        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)

        return {"download_url": download_url, "message": "å¯’å‡ä½œä¸šå¤„ç†å®Œæˆ"}

    except Exception as e:
        traceback.print_exc()
        return {"error": f"å¤„ç†å‡ºé”™: {str(e)}"}
