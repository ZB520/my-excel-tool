import pandas as pd
import re
import requests
import uuid
import os
import traceback
from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from io import BytesIO

app = FastAPI()

# æŒ‚è½½é™æ€ç›®å½•
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
def read_root():
    return {"message": "Service is running! All systems go."}

# =================================================================
# å…¬å…±å‡½æ•°ï¼šè‡ªåŠ¨è¯†åˆ«åˆ—å (Fuzzy Column Mapping)
# =================================================================
def find_columns_by_keywords(df_columns):
    """
    è¾“å…¥ DataFrame çš„åˆ—ååˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œæ˜ å°„æ ‡å‡†å­—æ®µååˆ°å®é™…åˆ—åã€‚
    """
    column_keywords = {
        'target_book_name': ['æ•™æ', 'ä¹¦å', 'åç§°', 'è¯¾æœ¬'],
        'target_publisher': ['å‡ºç‰ˆ', 'ç‰ˆç¤¾'],
        'target_isbn': ['ä¹¦å·', 'ISBN', 'isbn', 'æ ‡å‡†å·'],
        'target_class': ['ç­çº§', 'ä½¿ç”¨ç­çº§', 'é€‚ç”¨å¯¹è±¡', 'èŒƒå›´']
    }
    
    found_cols = {}
    for col in df_columns:
        col_str = str(col).strip()
        for key, keywords in column_keywords.items():
            if key not in found_cols and any(kw in col_str for kw in keywords):
                found_cols[key] = col
    
    return found_cols

# ==========================================
# ğŸšª ç¬¬ä¸€æ‰‡é—¨ï¼šå¤„ç†ã€ä¹¦å•ã€‘æ ¼å¼ (process_excel)
# ==========================================
@app.post("/process")
async def process_excel(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    
    if not file_url:
        return {"error": "No file_url provided"}
    
    try:
        response = requests.get(file_url)
        response.raise_for_status()
        file_content = BytesIO(response.content)
        
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        found_cols = find_columns_by_keywords(df.columns)
        
        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
             return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        def parse_class_info(class_str):
            classes = []
            pattern = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)äºº?ï¼‰'
            matches = re.findall(pattern, str(class_str))
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            pattern2 = r'(\d{2}[^ï¼ˆ\s]+?)ï¼ˆ(\d+)ï¼‰'
            matches2 = re.findall(pattern2, str(class_str))
            for match in matches2:
                if not any(c[0] == match[0] for c in classes):
                    classes.append((match[0], int(match[1])))
            return classes
        
        processed_data = []
        for index, row in df.iterrows():
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            classes = parse_class_info(str(class_str))
            for class_name, student_count in classes:
                processed_data.append({
                    'åŸå§‹ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })
        
        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "No valid data extracted"}

        def normalize_class_name_final(class_name):
            if 'äººï¼‰' in class_name or 'ï¼‰' in class_name:
                match = re.search(r'(2[45][^ï¼ˆï¼‰\s]+)', class_name)
                if match: return match.group(1)
            if 'çº§' in class_name and class_name.startswith(('24', '25')):
                year = class_name[:2]
                major = class_name[3:]
                if major.startswith('çº§'): major = major[1:]
                return year + major
            return class_name

        result_df['ç­çº§'] = result_df['åŸå§‹ç­çº§'].apply(normalize_class_name_final)
        
        result_df_unique = result_df.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()
        
        result_df_unique['å¹´ä»½'] = result_df_unique['ç­çº§'].str[:2]
        result_df_unique['ä¸“ä¸šç­çº§'] = result_df_unique['ç­çº§'].str[2:]
        result_df_sorted = result_df_unique.sort_values(['å¹´ä»½', 'ä¸“ä¸šç­çº§'], ascending=[False, True])
        
        # ==================== ä¿®å¤é€»è¾‘ START ====================
        # è·å–å”¯ä¸€çš„ç­çº§åˆ—è¡¨ï¼ˆä¿æŒæ’åºé¡ºåºï¼‰
        unique_classes = result_df_sorted['ç­çº§'].drop_duplicates().tolist()
        # åˆ›å»ºæ˜ å°„å­—å…¸ï¼š{ '24æŠ¤ç†1ç­': 1, '24æŠ¤ç†2ç­': 2, ... }
        class_map = {name: i for i, name in enumerate(unique_classes, 1)}
        # æ˜ å°„åºå·
        result_df_sorted['åºå·'] = result_df_sorted['ç­çº§'].map(class_map)
        # ==================== ä¿®å¤é€»è¾‘ END ======================
        
        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        for col in final_cols:
            if col not in result_df_sorted.columns:
                result_df_sorted[col] = ""
                
        final_df = result_df_sorted[final_cols]
        
        filename = f"result_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)
        
        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)
        
        return {"download_url": download_url, "message": "success"}
    
    except Exception as e:
        traceback.print_exc()
        return {"error": str(e)}


# ==========================================
# ğŸšª ç¬¬äºŒæ‰‡é—¨ï¼šå¤„ç†ã€å¯’å‡ä½œä¸šã€‘æ ¼å¼ (process_winter_homework)
# ==========================================
@app.post("/process_winter_homework")
async def process_winter_homework(request: Request):
    data = await request.json()
    file_url = data.get('file_url')
    if not file_url:
        return {"error": "è¯·æä¾›æ–‡ä»¶é“¾æ¥"}

    try:
        response = requests.get(file_url)
        file_content = BytesIO(response.content)
        
        df = pd.read_excel(file_content, sheet_name='Sheet1')
        found_cols = find_columns_by_keywords(df.columns)

        if 'target_class' not in found_cols or 'target_book_name' not in found_cols:
            return {"error": f"æ— æ³•è¯†åˆ«è¡¨å¤´ï¼Œè¯·ç¡®ä¿åŒ…å«'æ•™æåç§°'å’Œ'ä½¿ç”¨ç­çº§'ç›¸å…³åˆ—ã€‚è¯†åˆ«ç»“æœ: {list(df.columns)}"}

        def parse_class_info_new(class_str):
            classes = []
            s = str(class_str)
            pattern = r'(\d+ç­)\s*(\d+)äºº'
            matches = re.findall(pattern, s)
            for match in matches:
                classes.append((match[0], int(match[1])))
            
            if not classes:
                pattern2 = r'(\d+ç­)\s*(\d+)'
                matches2 = re.findall(pattern2, s)
                for match in matches2:
                    classes.append((match[0], int(match[1])))
            return classes

        processed_data = []
        for index, row in df.iterrows():
            textbook_name = row[found_cols['target_book_name']]
            class_str = row[found_cols['target_class']]
            publisher = row[found_cols['target_publisher']] if 'target_publisher' in found_cols else ""
            isbn = row[found_cols['target_isbn']] if 'target_isbn' in found_cols else ""

            if pd.isna(class_str) or str(class_str).strip() == '':
                continue
            
            classes = parse_class_info_new(class_str)
            for class_name, student_count in classes:
                processed_data.append({
                    'ç­çº§': class_name,
                    'ä¹¦å·': isbn,
                    'ä¹¦å': textbook_name,
                    'å‡ºç‰ˆç¤¾': publisher,
                    'å­¦ç”Ÿæ•°é‡': student_count
                })

        result_df = pd.DataFrame(processed_data)
        if result_df.empty:
            return {"error": "æœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç­çº§åˆ—æ ¼å¼"}

        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§'].astype(str).str.replace('ç­', '', regex=False)
        result_df = result_df[result_df['ç­çº§ç¼–å·æ•°å­—'].str.isnumeric()] 
        result_df['ç­çº§ç¼–å·æ•°å­—'] = result_df['ç­çº§ç¼–å·æ•°å­—'].astype(int)
        
        result_df_sorted = result_df.sort_values('ç­çº§ç¼–å·æ•°å­—', ascending=True)
        result_df_unique = result_df_sorted.drop_duplicates(subset=['ç­çº§', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'ä¹¦å·']).copy()

        # ==================== ä¿®å¤é€»è¾‘ START ====================
        # 1. æå–æ‰€æœ‰ä¸é‡å¤çš„ç­çº§ï¼Œä¿æŒæ’åºé¡ºåº
        unique_classes = result_df_unique['ç­çº§'].drop_duplicates().tolist()
        # 2. ç”Ÿæˆç­çº§IDå­—å…¸ï¼š{'101ç­': 1, '102ç­': 2, ...}
        class_map = {name: i for i, name in enumerate(unique_classes, 1)}
        # 3. å°†IDæ˜ å°„å›æ•°æ®æ¡†
        result_df_unique['åºå·'] = result_df_unique['ç­çº§'].map(class_map)
        # ==================== ä¿®å¤é€»è¾‘ END ======================

        final_cols = ['åºå·', 'ç­çº§', 'ä¹¦å·', 'ä¹¦å', 'å‡ºç‰ˆç¤¾', 'å­¦ç”Ÿæ•°é‡']
        for col in final_cols:
            if col not in result_df_unique.columns:
                result_df_unique[col] = ""

        final_df = result_df_unique[final_cols].reset_index(drop=True)

        filename = f"winter_hw_{uuid.uuid4()}.xlsx"
        save_path = os.path.join("static", filename)
        final_df.to_excel(save_path, index=False)

        base_url = str(request.base_url).rstrip("/")
        download_url = f"{base_url}/static/{filename}"
        if download_url.startswith("http://"):
            download_url = download_url.replace("http://", "https://", 1)

        return {"download_url": download_url, "message": "å¯’å‡ä½œä¸šå¤„ç†å®Œæˆ"}

    except Exception as e:
        traceback.print_exc()
        return {"error": f"å¤„ç†å‡ºé”™: {str(e)}"}
